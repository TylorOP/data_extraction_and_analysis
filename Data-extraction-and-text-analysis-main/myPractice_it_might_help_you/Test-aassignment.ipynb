{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c4a06c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import cmudict\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "import os\n",
    "import nltk\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b688d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/thispc/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package cmudict to /Users/thispc/nltk_data...\n",
      "[nltk_data]   Package cmudict is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/thispc/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('cmudict')\n",
    "nltk.download('stopwords')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d3b031",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bbc51de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Could not retrieve content from https://insights.blackcoffer.com/how-neural-networks-can-be-applied-in-various-areas-in-the-future/\n",
      "Error: Could not retrieve content from https://insights.blackcoffer.com/covid-19-environmental-impact-for-the-future/\n",
      "Error: Could not retrieve content from https://insights.blackcoffer.com/ensuring-growth-through-insurance-technology/\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import cmudict\n",
    "import os\n",
    "\n",
    "# Step 1: Data Extraction\n",
    "def extract_article_content(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        # Check if 'h1' element exists before accessing its 'text' attribute\n",
    "        article_title_element = soup.find('h1')\n",
    "        if article_title_element:\n",
    "            article_title = article_title_element.text.strip()\n",
    "        else:\n",
    "            article_title = \"No Title Found\"\n",
    "\n",
    "        # Find the article text\n",
    "        article_text_elements = soup.find_all('p')\n",
    "        article_text = '\\n'.join([p.text for p in article_text_elements])\n",
    "\n",
    "        return article_title, article_text\n",
    "    else:\n",
    "        print(f\"Error: Could not retrieve content from {url}\")\n",
    "        return None, None\n",
    "\n",
    "# Read input.xlsx to get the list of URLs and their corresponding URL_IDs\n",
    "input_file = \"Input.xlsx\"\n",
    "df_input = pd.read_excel(input_file)\n",
    "\n",
    "# Loop through each row in df_input to extract article content and save in text files\n",
    "for index, row in df_input.iterrows():\n",
    "    url_id = row[\"URL_ID\"]\n",
    "    url = row[\"URL\"]\n",
    "    article_title, article_text = extract_article_content(url)\n",
    "\n",
    "    if article_title and article_text:\n",
    "        # Save the article content in a text file\n",
    "        with open(f\"{url_id}.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(article_title + \"\\n\")\n",
    "            file.write(article_text)\n",
    "\n",
    "#         # Step 2: Data Analysis\n",
    "\n",
    "#         def perform_textual_analysis(article_text):\n",
    "#             if not article_text:\n",
    "#                 return {}  # Return an empty dictionary if the article_text is empty\n",
    "\n",
    "#             blob = TextBlob(article_text)\n",
    "#             positive_score = blob.sentiment.polarity\n",
    "#             negative_score = 1 - positive_score\n",
    "#             polarity_score = blob.sentiment.polarity\n",
    "#             subjectivity_score = blob.sentiment.subjectivity\n",
    "\n",
    "#             # Check if the article_text contains any words before calculating the average sentence length\n",
    "#             words = re.findall(r'\\w+', article_text)\n",
    "#             avg_sentence_length = len(blob.sentences) / len(words) if words else 0\n",
    "\n",
    "#             # Use the cmudict from nltk to count syllables of each word\n",
    "#             d = cmudict.dict()\n",
    "#             syllable_count = lambda word: max([len(list(y for y in x if y[-1].isdigit())) for x in d[word.lower()]])\n",
    "#             complex_words = [word for word in blob.words if syllable_count(word) > 2]\n",
    "#             percentage_complex_words = len(complex_words) / len(blob.words)\n",
    "#             fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
    "#             avg_words_per_sentence = len(blob.words) / len(blob.sentences)\n",
    "#             complex_word_count = len(complex_words)\n",
    "#             word_count = len(blob.words)\n",
    "#             syllables_per_word = blob.np_counts['syllables'] / blob.word_counts['word']\n",
    "#             personal_pronouns = len([word for word in blob.words if word.lower() in [\"i\", \"me\", \"my\", \"mine\", \"myself\"]])\n",
    "#             avg_word_length = sum(len(word) for word in blob.words) / len(blob.words)\n",
    "\n",
    "#             return {\n",
    "#                 \"POSITIVE SCORE\": positive_score,\n",
    "#                 \"NEGATIVE SCORE\": negative_score,\n",
    "#                 \"POLARITY SCORE\": polarity_score,\n",
    "#                 \"SUBJECTIVITY SCORE\": subjectivity_score,\n",
    "#                 \"AVG SENTENCE LENGTH\": avg_sentence_length,\n",
    "#                 \"PERCENTAGE OF COMPLEX WORDS\": percentage_complex_words,\n",
    "#                 \"FOG INDEX\": fog_index,\n",
    "#                 \"AVG NUMBER OF WORDS PER SENTENCE\": avg_words_per_sentence,\n",
    "#                 \"COMPLEX WORD COUNT\": complex_word_count,\n",
    "#                 \"WORD COUNT\": word_count,\n",
    "#                 \"SYLLABLE PER WORD\": syllables_per_word,\n",
    "#                 \"PERSONAL PRONOUNS\": personal_pronouns,\n",
    "#                 \"AVG WORD LENGTH\": avg_word_length\n",
    "#             }\n",
    "\n",
    "\n",
    "#         # Create an empty DataFrame to store the results of the analysis\n",
    "#         columns = [\"URL_ID\"] + list(perform_textual_analysis(\"\").keys())\n",
    "#         df_output = pd.DataFrame(columns=columns)\n",
    "\n",
    "#         # Loop through each row in df_input to perform textual analysis and store the results\n",
    "#         for index, row in df_input.iterrows():\n",
    "#             url_id = row[\"URL_ID\"]\n",
    "#             file_path = f\"{url_id}.txt\"\n",
    "\n",
    "#             # Read the article text from the text file\n",
    "#             with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "#                 article_text = file.read()\n",
    "\n",
    "#             # Perform textual analysis\n",
    "#             variables = perform_textual_analysis(article_text)\n",
    "#             variables[\"URL_ID\"] = url_id\n",
    "\n",
    "#             # Append the results to the output DataFrame\n",
    "#             df_output = df_output.append(variables, ignore_index=True)\n",
    "\n",
    "#         # Step 3: Save Output\n",
    "#         # Save the output DataFrame to a new Excel file\n",
    "#         output_file = \"output.xlsx\"\n",
    "#         df_output.to_excel(output_file, index=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9014c709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Could not retrieve content from https://insights.blackcoffer.com/how-neural-networks-can-be-applied-in-various-areas-in-the-future/\n",
      "Error: Could not retrieve content from https://insights.blackcoffer.com/covid-19-environmental-impact-for-the-future/\n",
      "Error: Could not retrieve content from https://insights.blackcoffer.com/ensuring-growth-through-insurance-technology/\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'StopWords/stop_words.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/28/xzg4vjpj3gj6x32fqvxyzd440000gn/T/ipykernel_2209/1613441176.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0mstop_words_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"stop_words.txt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0mstop_words_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstop_words_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_words_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m \u001b[0mstop_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_stop_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstop_words_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;31m# Step 3: Text Cleaning (Function from your previous code)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/28/xzg4vjpj3gj6x32fqvxyzd440000gn/T/ipykernel_2209/1613441176.py\u001b[0m in \u001b[0;36mload_stop_words\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;31m# Step 2: Loading Stop Words (Function from your previous code)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_stop_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0mstop_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'StopWords/stop_words.txt'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import cmudict\n",
    "import re\n",
    "\n",
    "# Step 1: Data Extraction \n",
    "def extract_article_content(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        # Check if 'h1' element exists before accessing its 'text' attribute\n",
    "        article_title_element = soup.find('h1')\n",
    "        if article_title_element:\n",
    "            article_title = article_title_element.text.strip()\n",
    "        else:\n",
    "            article_title = \"No Title Found\"\n",
    "\n",
    "        # Find the article text\n",
    "        article_text_elements = soup.find_all('p')\n",
    "        article_text = '\\n'.join([p.text for p in article_text_elements])\n",
    "\n",
    "        return article_title, article_text\n",
    "    else:\n",
    "        print(f\"Error: Could not retrieve content from {url}\")\n",
    "        return None, None\n",
    "\n",
    "# Read input.xlsx to get the list of URLs and their corresponding URL_IDs\n",
    "input_file = \"Input.xlsx\"\n",
    "df_input = pd.read_excel(input_file)\n",
    "\n",
    "# Loop through each row in df_input to extract article content and save in text files\n",
    "for index, row in df_input.iterrows():\n",
    "    url_id = row[\"URL_ID\"]\n",
    "    url = row[\"URL\"]\n",
    "    article_title, article_text = extract_article_content(url)\n",
    "\n",
    "    if article_title and article_text:\n",
    "        # Save the article content in a text file\n",
    "        with open(f\"{url_id}.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(article_title + \"\\n\")\n",
    "            file.write(article_text)\n",
    "            \n",
    "# Step 2: Loading Stop Words (Function from your previous code)\n",
    "def load_stop_words(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        stop_words = file.read().splitlines()\n",
    "    return stop_words\n",
    "\n",
    "stop_words_folder = \"StopWords\"\n",
    "stop_words_file = “StopWords_Auditor.txt”,“StopWords_Currencies.txt”,“StopWords_DatesandNumbers.txt”,“StopWords_GenericLong.txt”,“StopWords_Geographic.txt”,“StopWords_Names.txt”\n",
    "stop_words_path = os.path.join(stop_words_folder, stop_words_file)\n",
    "stop_words = load_stop_words(stop_words_path)\n",
    "\n",
    "# Step 3: Text Cleaning (Function from your previous code)\n",
    "def clean_text(text, stop_words):\n",
    "    # Remove punctuation\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    text = text.translate(translator)\n",
    "    \n",
    "    # Convert text to lowercase and split into words\n",
    "    words = text.lower().split()\n",
    "    \n",
    "    # Exclude stop words\n",
    "    cleaned_words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # Join cleaned words back to a sentence\n",
    "    cleaned_text = \" \".join(cleaned_words)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "# Step 4: Sentiment Analysis and Derived Variables\n",
    "# Define a function to calculate the positive and negative scores\n",
    "def calculate_positive_negative_scores(cleaned_text, positive_words, negative_words):\n",
    "    positive_score = sum(1 for word in cleaned_text.split() if word in positive_words)\n",
    "    negative_score = sum(1 for word in cleaned_text.split() if word in negative_words)\n",
    "    return positive_score, negative_score\n",
    "\n",
    "# Load the positive and negative words from the MasterDictionary folder\n",
    "master_dictionary_folder = \"MasterDictionary\"\n",
    "positive_words_file = \"positive_words.txt\"\n",
    "negative_words_file = \"negative_words.txt\"\n",
    "positive_words_path = os.path.join(master_dictionary_folder, positive_words_file)\n",
    "negative_words_path = os.path.join(master_dictionary_folder, negative_words_file)\n",
    "positive_words = load_stop_words(positive_words_path)\n",
    "negative_words = load_stop_words(negative_words_path)\n",
    "\n",
    "# Define a function to calculate the polarity score and subjectivity score\n",
    "def calculate_polarity_subjectivity_scores(positive_score, negative_score, total_words_after_cleaning):\n",
    "    polarity_score = (positive_score - negative_score) / (positive_score + negative_score + 0.000001)\n",
    "    subjectivity_score = (positive_score + negative_score) / (total_words_after_cleaning + 0.000001)\n",
    "    return polarity_score, subjectivity_score\n",
    "\n",
    "# Define a function to calculate the number of syllables in a word\n",
    "def count_syllables(word):\n",
    "    d = cmudict.dict()\n",
    "    if word.lower() in d:\n",
    "        return [len(list(y for y in x if y[-1].isdigit())) for x in d[word.lower()]][0]\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Define a function to calculate the average sentence length\n",
    "def calculate_average_sentence_length(cleaned_text):\n",
    "    sentences = nltk.sent_tokenize(cleaned_text)\n",
    "    total_words = sum(len(word_tokenize(sentence)) for sentence in sentences)\n",
    "    total_sentences = len(sentences)\n",
    "    return total_words / total_sentences\n",
    "\n",
    "# Define a function to calculate the percentage of complex words\n",
    "def calculate_complex_word_percentage(cleaned_text):\n",
    "    words = word_tokenize(cleaned_text)\n",
    "    complex_words = [word for word in words if count_syllables(word) > 2]\n",
    "    return len(complex_words) / len(words)\n",
    "\n",
    "# Define a function to calculate the Fog Index\n",
    "def calculate_fog_index(average_sentence_length, complex_word_percentage):\n",
    "    return 0.4 * (average_sentence_length + complex_word_percentage)\n",
    "\n",
    "# Define a function to calculate the average number of words per sentence\n",
    "def calculate_average_words_per_sentence(cleaned_text):\n",
    "    sentences = nltk.sent_tokenize(cleaned_text)\n",
    "    total_words = sum(len(word_tokenize(sentence)) for sentence in sentences)\n",
    "    total_sentences = len(sentences)\n",
    "    return total_words / total_sentences\n",
    "\n",
    "# Define a function to count the number of personal pronouns\n",
    "def count_personal_pronouns(cleaned_text):\n",
    "    pronoun_regex = r'\\b(?:I|we|my|ours|us)\\b'\n",
    "    return len(re.findall(pronoun_regex, cleaned_text, re.IGNORECASE))\n",
    "\n",
    "# Define a function to calculate the average word length\n",
    "def calculate_average_word_length(cleaned_text):\n",
    "    words = word_tokenize(cleaned_text)\n",
    "    total_characters = sum(len(word) for word in words)\n",
    "    return total_characters / len(words)\n",
    "\n",
    "# Continue with your existing code for data extraction and saving to text files\n",
    "# ... (Paste your existing code here)\n",
    "\n",
    "# Calculate and save the derived variables for each article text\n",
    "for index, row in df_input.iterrows():\n",
    "    url_id = row[\"URL_ID\"]\n",
    "    url = row[\"URL\"]\n",
    "    article_title, article_text = extract_article_content(url)\n",
    "\n",
    "    if article_title and article_text:\n",
    "        # Save the article content in a text file\n",
    "        with open(f\"{url_id}.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(article_title + \"\\n\")\n",
    "            file.write(article_text)\n",
    "\n",
    "        # Read the content from the text file\n",
    "        with open(f\"{url_id}.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "            text = file.read()\n",
    "\n",
    "        # Clean the text using stop words\n",
    "        cleaned_text = clean_text(text, stop_words)\n",
    "\n",
    "        # Calculate the positive and negative scores\n",
    "        positive_score, negative_score = calculate_positive_negative_scores(cleaned_text, positive_words, negative_words)\n",
    "\n",
    "        # Calculate the total words after cleaning\n",
    "        total_words_after_cleaning = len(cleaned_text.split())\n",
    "\n",
    "        # Calculate the polarity score and subjectivity score\n",
    "        polarity_score, subjectivity_score = calculate_polarity_subjectivity_scores(positive_score, negative_score, total_words_after_cleaning)\n",
    "\n",
    "        # Calculate the average sentence length\n",
    "        average_sentence_length = calculate_average_sentence_length(cleaned_text)\n",
    "\n",
    "        # Calculate the percentage of complex words\n",
    "        complex_word_percentage = calculate_complex_word_percentage(cleaned_text)\n",
    "\n",
    "        # Calculate the Fog Index\n",
    "        fog_index = calculate_fog_index(average_sentence_length, complex_word_percentage)\n",
    "\n",
    "        # Calculate the average number of words per sentence\n",
    "        average_words_per_sentence = calculate_average_words_per_sentence(cleaned_text)\n",
    "\n",
    "        # Calculate the complex word count\n",
    "        complex_word_count = len([word for word in cleaned_text.split() if count_syllables(word) > 2])\n",
    "\n",
    "        # Calculate the word count\n",
    "        word_count = len(cleaned_text.split())\n",
    "\n",
    "        # Calculate the syllable count per word\n",
    "        syllable_count_per_word = sum(count_syllables(word) for word in cleaned_text.split()) / word_count\n",
    "\n",
    "        # Calculate the personal pronouns count\n",
    "        personal_pronouns_count = count_personal_pronouns(cleaned_text)\n",
    "\n",
    "        # Calculate the average word length\n",
    "        average_word_length = calculate_average_word_length(cleaned_text)\n",
    "\n",
    "        # Save the derived variables to a excel file (you can use pandas DataFrame)\n",
    "        output_df = pd.DataFrame(output_data, columns=[\n",
    "            \"URL_ID\": [url_id],\n",
    "            \"URL\": [url],\n",
    "            \"Positive_Score\": [positive_score],\n",
    "            \"Negative_Score\": [negative_score],\n",
    "            \"Polarity_Score\": [polarity_score],\n",
    "            \"Subjectivity_Score\": [subjectivity_score],\n",
    "            \"Average_Sentence_Length\": [average_sentence_length],\n",
    "            \"Complex_Word_Percentage\": [complex_word_percentage],\n",
    "            \"Fog_Index\": [fog_index],\n",
    "            \"Average_Words_Per_Sentence\": [average_words_per_sentence],\n",
    "            \"Complex_Word_Count\": [complex_word_count],\n",
    "            \"Word_Count\": [word_count],\n",
    "            \"Syllable_Count_Per_Word\": [syllable_count_per_word],\n",
    "            \"Personal_Pronouns_Count\": [personal_pronouns_count],\n",
    "            \"Average_Word_Length\": [average_word_length]\n",
    "        ])\n",
    "\n",
    "       \n",
    "        # Save the DataFrame to \"Output Data Structure.xlsx\"\n",
    "        output_file = \"Output Data Structure.xlsx\"\n",
    "        output_df.to_excel(output_file, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ea793372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['StopWords_Names.txt', 'StopWords_Auditor.txt', 'StopWords_Geographic.txt', 'StopWords_Generic.txt', 'StopWords_Currencies.txt', 'StopWords_DatesandNumbers.txt', 'StopWords_GenericLong.txt']\n"
     ]
    }
   ],
   "source": [
    "stop_words_folder = \"StopWords\"\n",
    "print(os.listdir(stop_words_folder))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef9f3c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e2094a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2342eab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Loading Stop Words (Modified Function)\n",
    "def load_stop_words(file_path, encodings=['utf-8', 'latin-1']):\n",
    "    stop_words = set()\n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding=encoding) as file:\n",
    "                words = file.read().splitlines()\n",
    "                stop_words.update(words)\n",
    "            break  # Stop if successful\n",
    "        except UnicodeDecodeError:\n",
    "            continue  # Try the next encoding if there's an error\n",
    "    return stop_words\n",
    "\n",
    "# Example usage:\n",
    "master_dictionary_folder = \"MasterDictionary\"\n",
    "positive_words_file = \"positive-words.txt\"\n",
    "negative_words_file = \"negative-words.txt\"\n",
    "\n",
    "positive_words_path = os.path.join(master_dictionary_folder, positive_words_file)\n",
    "negative_words_path = os.path.join(master_dictionary_folder, negative_words_file)\n",
    "\n",
    "positive_words = load_stop_words(positive_words_path)\n",
    "negative_words = load_stop_words(negative_words_path)\n",
    "\n",
    "\n",
    "\n",
    "# Step 3: Text Cleaning (Function from your previous code)\n",
    "def clean_text(text, stop_words):\n",
    "    # Remove punctuation\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    text = text.translate(translator)\n",
    "    \n",
    "    # Convert text to lowercase and split into words\n",
    "    words = text.lower().split()\n",
    "    \n",
    "    # Exclude stop words\n",
    "    cleaned_words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # Join cleaned words back to a sentence\n",
    "    cleaned_text = \" \".join(cleaned_words)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "# Step 4: Sentiment Analysis and Derived Variables\n",
    "\n",
    "# Define a function to calculate the positive and negative scores\n",
    "def calculate_positive_negative_scores(cleaned_text, positive_words, negative_words):\n",
    "    positive_score = sum(1 for word in cleaned_text.split() if word in positive_words)\n",
    "    negative_score = sum(1 for word in cleaned_text.split() if word in negative_words)\n",
    "    return positive_score, negative_score\n",
    "\n",
    "# Load the positive and negative words from the MasterDictionary folder\n",
    "master_dictionary_folder = \"MasterDictionary\"\n",
    "positive_words_file = \"positive-words.txt\"\n",
    "negative_words_file = \"negative-words.txt\"\n",
    "positive_words_path = os.path.join(master_dictionary_folder, positive_words_file)\n",
    "negative_words_path = os.path.join(master_dictionary_folder, negative_words_file)\n",
    "positive_words = load_stop_words(positive_words_path)\n",
    "negative_words = load_stop_words(negative_words_path)\n",
    "\n",
    "# Define a function to calculate the polarity score and subjectivity score\n",
    "def calculate_polarity_subjectivity_scores(positive_score, negative_score, total_words_after_cleaning):\n",
    "    polarity_score = (positive_score - negative_score) / (positive_score + negative_score + 0.000001)\n",
    "    subjectivity_score = (positive_score + negative_score) / (total_words_after_cleaning + 0.000001)\n",
    "    return polarity_score, subjectivity_score\n",
    "\n",
    "# Define a function to calculate the number of syllables in a word\n",
    "def count_syllables(word):\n",
    "    d = cmudict.dict()\n",
    "    if word.lower() in d:\n",
    "        return [len(list(y for y in x if y[-1].isdigit())) for x in d[word.lower()]][0]\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Define a function to calculate the average sentence length\n",
    "def calculate_average_sentence_length(cleaned_text):\n",
    "    sentences = nltk.sent_tokenize(cleaned_text)\n",
    "    total_words = sum(len(word_tokenize(sentence)) for sentence in sentences)\n",
    "    total_sentences = len(sentences)\n",
    "    return total_words / total_sentences\n",
    "\n",
    "# Define a function to calculate the percentage of complex words\n",
    "def calculate_complex_word_percentage(cleaned_text):\n",
    "    words = word_tokenize(cleaned_text)\n",
    "    complex_words = [word for word in words if count_syllables(word) > 2]\n",
    "    return len(complex_words) / len(words)\n",
    "\n",
    "# Define a function to calculate the Fog Index\n",
    "def calculate_fog_index(average_sentence_length, complex_word_percentage):\n",
    "    return 0.4 * (average_sentence_length + complex_word_percentage)\n",
    "\n",
    "# Define a function to calculate the average number of words per sentence\n",
    "def calculate_average_words_per_sentence(cleaned_text):\n",
    "    sentences = nltk.sent_tokenize(cleaned_text)\n",
    "    total_words = sum(len(word_tokenize(sentence)) for sentence in sentences)\n",
    "    total_sentences = len(sentences)\n",
    "    return total_words / total_sentences\n",
    "\n",
    "# Define a function to count the number of personal pronouns\n",
    "def count_personal_pronouns(cleaned_text):\n",
    "    pronoun_regex = r'\\b(?:I|we|my|ours|us)\\b'\n",
    "    return len(re.findall(pronoun_regex, cleaned_text, re.IGNORECASE))\n",
    "\n",
    "# Define a function to calculate the average word length\n",
    "def calculate_average_word_length(cleaned_text):\n",
    "    words = word_tokenize(cleaned_text)\n",
    "    total_characters = sum(len(word) for word in words)\n",
    "    return total_characters / len(words)\n",
    "\n",
    "# Continue with your existing code for data extraction and saving to text files\n",
    "# ... (Paste your existing code here)\n",
    "\n",
    "# Create an empty list to store the derived variables for each URL\n",
    "output_data = []\n",
    "\n",
    "# Calculate and save the derived variables for each article text\n",
    "for index, row in df_input.iterrows():\n",
    "    url_id = row[\"URL_ID\"]\n",
    "    url = row[\"URL\"]\n",
    "    article_title, article_text = extract_article_content(url)\n",
    "\n",
    "    if article_title and article_text:\n",
    "        # Read the content from the text file\n",
    "        with open(f\"{url_id}.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "            text = file.read()\n",
    "\n",
    "        # Clean the text using stop words\n",
    "        cleaned_text = clean_text(text, stop_words)\n",
    "\n",
    "        # Calculate the positive and negative scores\n",
    "        positive_score, negative_score = calculate_positive_negative_scores(cleaned_text, positive_words, negative_words)\n",
    "\n",
    "        # Calculate the total words after cleaning\n",
    "        total_words_after_cleaning = len(cleaned_text.split())\n",
    "\n",
    "        # Calculate the polarity score and subjectivity score\n",
    "        polarity_score, subjectivity_score = calculate_polarity_subjectivity_scores(positive_score, negative_score, total_words_after_cleaning)\n",
    "\n",
    "        # Calculate the average sentence length\n",
    "        average_sentence_length = calculate_average_sentence_length(cleaned_text)\n",
    "\n",
    "        # Calculate the percentage of complex words\n",
    "        complex_word_percentage = calculate_complex_word_percentage(cleaned_text)\n",
    "\n",
    "        # Calculate the Fog Index\n",
    "        fog_index = calculate_fog_index(average_sentence_length, complex_word_percentage)\n",
    "\n",
    "        # Calculate the average number of words per sentence\n",
    "        average_words_per_sentence = calculate_average_words_per_sentence(cleaned_text)\n",
    "\n",
    "        # Calculate the complex word count\n",
    "        complex_word_count = len([word for word in cleaned_text.split() if count_syllables(word) > 2])\n",
    "\n",
    "        # Calculate the word count\n",
    "        word_count = len(cleaned_text.split())\n",
    "\n",
    "        # Calculate the syllable count per word\n",
    "        syllable_count_per_word = sum(count_syllables(word) for word in cleaned_text.split()) / word_count\n",
    "\n",
    "        # Calculate the personal pronouns count\n",
    "        personal_pronouns_count = count_personal_pronouns(cleaned_text)\n",
    "\n",
    "        # Calculate the average word length\n",
    "        average_word_length = calculate_average_word_length(cleaned_text)\n",
    "\n",
    "         # Create a dictionary to store the derived variables for this URL\n",
    "            url_data = {\n",
    "            \"URL_ID\": url_id,\n",
    "            \"URL\": url,\n",
    "            \"Positive Score\": positive_score,\n",
    "            \"Negative Score\": negative_score,\n",
    "            \"Polarity Score\": polarity_score,\n",
    "            \"Subjectivity Score\": subjectivity_score,\n",
    "            \"Avg Sentence Length\": average_sentence_length,\n",
    "            \"Percentage of Complex Words\": percentage_complex_words,\n",
    "            \"Fog Index\": fog_index,\n",
    "            \"Avg Number of Words Per Sentence\": avg_words_per_sentence,\n",
    "            \"Complex Word Count\": complex_word_count,\n",
    "            \"Word Count\": word_count,\n",
    "            \"Syllable Per Word\": syllable_per_word,\n",
    "            \"Personal Pronouns\": personal_pronouns_count,\n",
    "            \"Avg Word Length\": avg_word_length\n",
    "        }\n",
    "        # Append the dictionary to the output_data list\n",
    "        output_data.append(url_data)\n",
    "       \n",
    "       # Step 5: Output Data to Excel (Continued from your previous code)\n",
    "\n",
    "# Create a DataFrame from the output_data list\n",
    "output_df = pd.DataFrame(output_data)\n",
    "\n",
    "# Save the DataFrame to an Excel file\n",
    "output_file = \"Output.xlsx\"\n",
    "output_df.to_excel(output_file, index=False)\n",
    "\n",
    "# End of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c2852b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ae48a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9504907b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e746104e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d2bc25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e953d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78b4473",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2f6d2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31879793",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1018407",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d79dc8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb34a301",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424c3033",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62f8fb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307f40cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1e57cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec67226",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514eb742",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a3833e59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Could not retrieve content from https://insights.blackcoffer.com/how-neural-networks-can-be-applied-in-various-areas-in-the-future/\n",
      "Error: Could not retrieve content from https://insights.blackcoffer.com/covid-19-environmental-impact-for-the-future/\n",
      "Error: Could not retrieve content from https://insights.blackcoffer.com/ensuring-growth-through-insurance-technology/\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'nlp-based'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/28/xzg4vjpj3gj6x32fqvxyzd440000gn/T/ipykernel_2138/3186974917.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;31m# Perform textual analysis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m     \u001b[0mvariables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperform_textual_analysis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticle_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m     \u001b[0mvariables\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"URL_ID\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murl_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/28/xzg4vjpj3gj6x32fqvxyzd440000gn/T/ipykernel_2138/3186974917.py\u001b[0m in \u001b[0;36mperform_textual_analysis\u001b[0;34m(article_text)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcmudict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0msyllable_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdigit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m     \u001b[0mcomplex_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mblob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msyllable_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m     \u001b[0mpercentage_complex_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomplex_words\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0mfog_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.4\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mavg_sentence_length\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpercentage_complex_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/28/xzg4vjpj3gj6x32fqvxyzd440000gn/T/ipykernel_2138/3186974917.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcmudict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0msyllable_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdigit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m     \u001b[0mcomplex_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mblob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msyllable_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m     \u001b[0mpercentage_complex_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomplex_words\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0mfog_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.4\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mavg_sentence_length\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpercentage_complex_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/28/xzg4vjpj3gj6x32fqvxyzd440000gn/T/ipykernel_2138/3186974917.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(word)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;31m# Use the cmudict from nltk to count syllables of each word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcmudict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0msyllable_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdigit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0mcomplex_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mblob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msyllable_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0mpercentage_complex_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomplex_words\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'nlp-based'"
     ]
    }
   ],
   "source": [
    "#original\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import cmudict\n",
    "\n",
    "# Step 1: Data Extraction\n",
    "def extract_article_content(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        # Check if 'h1' element exists before accessing its 'text' attribute\n",
    "        article_title_element = soup.find('h1')\n",
    "        if article_title_element:\n",
    "            article_title = article_title_element.text.strip()\n",
    "        else:\n",
    "            article_title = \"No Title Found\"\n",
    "\n",
    "        # Find the article text\n",
    "        article_text_elements = soup.find_all('p')\n",
    "        article_text = '\\n'.join([p.text for p in article_text_elements])\n",
    "\n",
    "        return article_title, article_text\n",
    "    else:\n",
    "        print(f\"Error: Could not retrieve content from {url}\")\n",
    "        return None, None\n",
    "\n",
    "# Read input.xlsx to get the list of URLs and their corresponding URL_IDs\n",
    "input_file = \"Input.xlsx\"\n",
    "df_input = pd.read_excel(input_file)\n",
    "\n",
    "# Loop through each row in df_input to extract article content and save in text files\n",
    "for index, row in df_input.iterrows():\n",
    "    url_id = row[\"URL_ID\"]\n",
    "    url = row[\"URL\"]\n",
    "    article_title, article_text = extract_article_content(url)\n",
    "\n",
    "    if article_title and article_text:\n",
    "        # Save the article content in a text file\n",
    "        with open(f\"{url_id}.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(article_title + \"\\n\")\n",
    "            file.write(article_text)\n",
    "\n",
    "# Step 2: Data Analysis\n",
    "\n",
    "def perform_textual_analysis(article_text):\n",
    "    if not article_text:\n",
    "        return {}  # Return an empty dictionary if the article_text is empty\n",
    "\n",
    "    blob = TextBlob(article_text)\n",
    "    positive_score = blob.sentiment.polarity\n",
    "    negative_score = 1 - positive_score\n",
    "    polarity_score = blob.sentiment.polarity\n",
    "    subjectivity_score = blob.sentiment.subjectivity\n",
    "\n",
    "    # Check if the article_text contains any words before calculating the average sentence length\n",
    "    words = re.findall(r'\\w+', article_text)\n",
    "    avg_sentence_length = len(blob.sentences) / len(words) if words else 0\n",
    "\n",
    "    # Use the cmudict from nltk to count syllables of each word\n",
    "    d = cmudict.dict()\n",
    "    syllable_count = lambda word: max([len(list(y for y in x if y[-1].isdigit())) for x in d[word.lower()]])\n",
    "    complex_words = [word for word in blob.words if syllable_count(word) >= 3]\n",
    "    percentage_complex_words = len(complex_words) / len(blob.words)\n",
    "    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
    "    avg_words_per_sentence = len(blob.words) / len(blob.sentences)\n",
    "    complex_word_count = len(complex_words)\n",
    "    word_count = len(blob.words)\n",
    "    syllables_per_word = blob.np_counts['syllables'] / blob.word_counts['word']\n",
    "    personal_pronouns = len([word for word in blob.words if word.lower() in [\"i\", \"me\", \"my\", \"mine\", \"myself\"]])\n",
    "    avg_word_length = sum(len(word) for word in blob.words) / len(blob.words)\n",
    "\n",
    "    return {\n",
    "        \"POSITIVE SCORE\": positive_score,\n",
    "        \"NEGATIVE SCORE\": negative_score,\n",
    "        \"POLARITY SCORE\": polarity_score,\n",
    "        \"SUBJECTIVITY SCORE\": subjectivity_score,\n",
    "        \"AVG SENTENCE LENGTH\": avg_sentence_length,\n",
    "        \"PERCENTAGE OF COMPLEX WORDS\": percentage_complex_words,\n",
    "        \"FOG INDEX\": fog_index,\n",
    "        \"AVG NUMBER OF WORDS PER SENTENCE\": avg_words_per_sentence,\n",
    "        \"COMPLEX WORD COUNT\": complex_word_count,\n",
    "        \"WORD COUNT\": word_count,\n",
    "        \"SYLLABLE PER WORD\": syllables_per_word,\n",
    "        \"PERSONAL PRONOUNS\": personal_pronouns,\n",
    "        \"AVG WORD LENGTH\": avg_word_length\n",
    "    }\n",
    "\n",
    "\n",
    "# Create an empty DataFrame to store the results of the analysis\n",
    "columns = [\"URL_ID\"] + list(perform_textual_analysis(\"\").keys())\n",
    "df_output = pd.DataFrame(columns=columns)\n",
    "\n",
    "# Loop through each row in df_input to perform textual analysis and store the results\n",
    "for index, row in df_input.iterrows():\n",
    "    url_id = row[\"URL_ID\"]\n",
    "    file_path = f\"{url_id}.txt\"\n",
    "\n",
    "    # Read the article text from the text file\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        article_text = file.read()\n",
    "\n",
    "    # Perform textual analysis\n",
    "    variables = perform_textual_analysis(article_text)\n",
    "    variables[\"URL_ID\"] = url_id\n",
    "\n",
    "    # Append the results to the output DataFrame\n",
    "    df_output = df_output.append(variables, ignore_index=True)\n",
    "\n",
    "# Step 3: Save Output\n",
    "# Save the output DataFrame to a new Excel file\n",
    "output_file = \"output.xlsx\"\n",
    "df_output.to_excel(output_file, index=False)\n",
    "\n",
    "# Step 4: Submission\n",
    "# Follow the submission instructions provided in the assignment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4d1893ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe249c5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4134a6dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "add861f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac00443",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f7ac49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafaf754",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68ca8f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e57b61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b309e006",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d14d9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be00e54b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb6dcab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c14933d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a108b18f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee910d6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d79db42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee39f62f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43071dba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8a4eda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb21a42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1307e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57308f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb863b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11b0e58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2246ed9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#got error in last line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58965da9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_input' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/28/xzg4vjpj3gj6x32fqvxyzd440000gn/T/ipykernel_1603/444907365.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;31m# Calculate and save the derived variables for each article text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m     \u001b[0murl_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"URL_ID\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"URL\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_input' is not defined"
     ]
    }
   ],
   "source": [
    "# Step 2: Loading Stop Words (Modified Function)\n",
    "def load_stop_words(file_path, encodings=['utf-8', 'latin-1']):\n",
    "    stop_words = set()\n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding=encoding) as file:\n",
    "                words = file.read().splitlines()\n",
    "                stop_words.update(words)\n",
    "            break  # Stop if successful\n",
    "        except UnicodeDecodeError:\n",
    "            continue  # Try the next encoding if there's an error\n",
    "    return stop_words\n",
    "\n",
    "# Example usage:\n",
    "master_dictionary_folder = \"MasterDictionary\"\n",
    "positive_words_file = \"positive-words.txt\"\n",
    "negative_words_file = \"negative-words.txt\"\n",
    "\n",
    "positive_words_path = os.path.join(master_dictionary_folder, positive_words_file)\n",
    "negative_words_path = os.path.join(master_dictionary_folder, negative_words_file)\n",
    "\n",
    "positive_words = load_stop_words(positive_words_path)\n",
    "negative_words = load_stop_words(negative_words_path)\n",
    "\n",
    "\n",
    "\n",
    "# Step 3: Text Cleaning (Function from your previous code)\n",
    "def clean_text(text, stop_words):\n",
    "    # Remove punctuation\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    text = text.translate(translator)\n",
    "    \n",
    "    # Convert text to lowercase and split into words\n",
    "    words = text.lower().split()\n",
    "    \n",
    "    # Exclude stop words\n",
    "    cleaned_words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # Join cleaned words back to a sentence\n",
    "    cleaned_text = \" \".join(cleaned_words)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "# Step 4: Sentiment Analysis and Derived Variables\n",
    "# Define a function to calculate the positive and negative scores\n",
    "def calculate_positive_negative_scores(cleaned_text, positive_words, negative_words):\n",
    "    positive_score = sum(1 for word in cleaned_text.split() if word in positive_words)\n",
    "    negative_score = sum(1 for word in cleaned_text.split() if word in negative_words)\n",
    "    return positive_score, negative_score\n",
    "\n",
    "# Load the positive and negative words from the MasterDictionary folder\n",
    "master_dictionary_folder = \"MasterDictionary\"\n",
    "positive_words_file = \"positive-words.txt\"\n",
    "negative_words_file = \"negative-words.txt\"\n",
    "positive_words_path = os.path.join(master_dictionary_folder, positive_words_file)\n",
    "negative_words_path = os.path.join(master_dictionary_folder, negative_words_file)\n",
    "positive_words = load_stop_words(positive_words_path)\n",
    "negative_words = load_stop_words(negative_words_path)\n",
    "\n",
    "# Define a function to calculate the polarity score and subjectivity score\n",
    "def calculate_polarity_subjectivity_scores(positive_score, negative_score, total_words_after_cleaning):\n",
    "    polarity_score = (positive_score - negative_score) / (positive_score + negative_score + 0.000001)\n",
    "    subjectivity_score = (positive_score + negative_score) / (total_words_after_cleaning + 0.000001)\n",
    "    return polarity_score, subjectivity_score\n",
    "\n",
    "# Define a function to calculate the number of syllables in a word\n",
    "def count_syllables(word):\n",
    "    d = cmudict.dict()\n",
    "    if word.lower() in d:\n",
    "        return [len(list(y for y in x if y[-1].isdigit())) for x in d[word.lower()]][0]\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Define a function to calculate the average sentence length\n",
    "def calculate_average_sentence_length(cleaned_text):\n",
    "    sentences = nltk.sent_tokenize(cleaned_text)\n",
    "    total_words = sum(len(word_tokenize(sentence)) for sentence in sentences)\n",
    "    total_sentences = len(sentences)\n",
    "    return total_words / total_sentences\n",
    "\n",
    "# Define a function to calculate the percentage of complex words\n",
    "def calculate_complex_word_percentage(cleaned_text):\n",
    "    words = word_tokenize(cleaned_text)\n",
    "    complex_words = [word for word in words if count_syllables(word) > 2]\n",
    "    return len(complex_words) / len(words)\n",
    "\n",
    "# Define a function to calculate the Fog Index\n",
    "def calculate_fog_index(average_sentence_length, complex_word_percentage):\n",
    "    return 0.4 * (average_sentence_length + complex_word_percentage)\n",
    "\n",
    "# Define a function to calculate the average number of words per sentence\n",
    "def calculate_average_words_per_sentence(cleaned_text):\n",
    "    sentences = nltk.sent_tokenize(cleaned_text)\n",
    "    total_words = sum(len(word_tokenize(sentence)) for sentence in sentences)\n",
    "    total_sentences = len(sentences)\n",
    "    return total_words / total_sentences\n",
    "\n",
    "# Define a function to count the number of personal pronouns\n",
    "def count_personal_pronouns(cleaned_text):\n",
    "    pronoun_regex = r'\\b(?:I|we|my|ours|us)\\b'\n",
    "    return len(re.findall(pronoun_regex, cleaned_text, re.IGNORECASE))\n",
    "\n",
    "# Define a function to calculate the average word length\n",
    "def calculate_average_word_length(cleaned_text):\n",
    "    words = word_tokenize(cleaned_text)\n",
    "    total_characters = sum(len(word) for word in words)\n",
    "    return total_characters / len(words)\n",
    "\n",
    "# Continue with your existing code for data extraction and saving to text files\n",
    "# ... (Paste your existing code here)\n",
    "\n",
    "# Calculate and save the derived variables for each article text\n",
    "for index, row in df_input.iterrows():\n",
    "    url_id = row[\"URL_ID\"]\n",
    "    url = row[\"URL\"]\n",
    "    article_title, article_text = extract_article_content(url)\n",
    "\n",
    "    if article_title and article_text:\n",
    "        # Save the article content in a text file\n",
    "        with open(f\"{url_id}.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(article_title + \"\\n\")\n",
    "            file.write(article_text)\n",
    "\n",
    "        # Read the content from the text file\n",
    "        with open(f\"{url_id}.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "            text = file.read()\n",
    "\n",
    "        # Clean the text using stop words\n",
    "        cleaned_text = clean_text(text, stop_words)\n",
    "\n",
    "        # Calculate the positive and negative scores\n",
    "        positive_score, negative_score = calculate_positive_negative_scores(cleaned_text, positive_words, negative_words)\n",
    "\n",
    "        # Calculate the total words after cleaning\n",
    "        total_words_after_cleaning = len(cleaned_text.split())\n",
    "\n",
    "        # Calculate the polarity score and subjectivity score\n",
    "        polarity_score, subjectivity_score = calculate_polarity_subjectivity_scores(positive_score, negative_score, total_words_after_cleaning)\n",
    "\n",
    "        # Calculate the average sentence length\n",
    "        average_sentence_length = calculate_average_sentence_length(cleaned_text)\n",
    "\n",
    "        # Calculate the percentage of complex words\n",
    "        complex_word_percentage = calculate_complex_word_percentage(cleaned_text)\n",
    "\n",
    "        # Calculate the Fog Index\n",
    "        fog_index = calculate_fog_index(average_sentence_length, complex_word_percentage)\n",
    "\n",
    "        # Calculate the average number of words per sentence\n",
    "        average_words_per_sentence = calculate_average_words_per_sentence(cleaned_text)\n",
    "\n",
    "        # Calculate the complex word count\n",
    "        complex_word_count = len([word for word in cleaned_text.split() if count_syllables(word) > 2])\n",
    "\n",
    "        # Calculate the word count\n",
    "        word_count = len(cleaned_text.split())\n",
    "\n",
    "        # Calculate the syllable count per word\n",
    "        syllable_count_per_word = sum(count_syllables(word) for word in cleaned_text.split()) / word_count\n",
    "\n",
    "        # Calculate the personal pronouns count\n",
    "        personal_pronouns_count = count_personal_pronouns(cleaned_text)\n",
    "\n",
    "        # Calculate the average word length\n",
    "        average_word_length = calculate_average_word_length(cleaned_text)\n",
    "\n",
    "        # Save the derived variables to a excel file (you can use pandas DataFrame)\n",
    "        output_df = pd.DataFrame(output_data, columns={\n",
    "           \"URL_ID\": url_id,\n",
    "            \"URL\": url,\n",
    "            \"Positive Score\": positive_score,\n",
    "            \"Negative Score\": negative_score,\n",
    "            \"Polarity Score\": polarity_score,\n",
    "            \"Subjectivity Score\": subjectivity_score,\n",
    "            \"Avg Sentence Length\": average_sentence_length,\n",
    "            \"Percentage of Complex Words\": complex_word_percentage,\n",
    "            \"Fog Index\": fog_index,\n",
    "            \"Avg Number of Words Per Sentence\": average_words_per_sentence,\n",
    "            \"Complex Word Count\": complex_word_count,\n",
    "            \"Word Count\": word_count,\n",
    "            \"Syllable Per Word\": syllable_count_per_word,\n",
    "            \"Personal Pronouns\": personal_pronouns_count,\n",
    "            \"Avg Word Length\": average_word_length\n",
    "            })\n",
    "\n",
    "       \n",
    "        # Save the DataFrame to \"Output Data Structure.xlsx\"\n",
    "        output_file = \"Output Data Structure.xlsx\"\n",
    "        output_df.to_excel(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff8f418",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
