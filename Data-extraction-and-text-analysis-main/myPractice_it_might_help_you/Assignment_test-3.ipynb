{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80361c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#just solution\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import cmudict\n",
    "import os\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6343ee6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read input.xlsx to get the list of URLs and their corresponding URL_IDs(upload you input file in home of jupyter)\n",
    "input_file = \"Input.xlsx\"\n",
    "df_input = pd.read_excel(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0da9ea86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Data Extraction\n",
    "def extract_article_content(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        # Check if 'h1' element exists before accessing its 'text' attribute\n",
    "        article_title_element = soup.find('h1')\n",
    "        if article_title_element:\n",
    "            article_title = article_title_element.text.strip()\n",
    "        else:\n",
    "            article_title = \"No Title Found\"\n",
    "\n",
    "        # Find the article text\n",
    "        article_text_elements = soup.find_all('p')\n",
    "        article_text = '\\n'.join([p.text for p in article_text_elements])\n",
    "\n",
    "        return article_title, article_text\n",
    "    else:\n",
    "        print(f\"Error: Could not retrieve content from {url}\")\n",
    "        return None, None\n",
    "\n",
    "\n",
    "#done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15754c57",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Could not retrieve content from https://insights.blackcoffer.com/how-neural-networks-can-be-applied-in-various-areas-in-the-future/\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/28/xzg4vjpj3gj6x32fqvxyzd440000gn/T/ipykernel_2432/1858443399.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0murl_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"URL_ID\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"URL\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0marticle_title\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marticle_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_article_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0marticle_title\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marticle_text\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/28/xzg4vjpj3gj6x32fqvxyzd440000gn/T/ipykernel_2432/3414893912.py\u001b[0m in \u001b[0;36mextract_article_content\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Step 1: Data Extraction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mextract_article_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'html.parser'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \"\"\"\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'get'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    540\u001b[0m         }\n\u001b[1;32m    541\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    696\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 697\u001b[0;31m             \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/requests/models.py\u001b[0m in \u001b[0;36mcontent\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    834\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 836\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mb''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCONTENT_CHUNK_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34mb''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_content_consumed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/requests/models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'stream'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 758\u001b[0;31m                     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    759\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mProtocolError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/urllib3/response.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \"\"\"\n\u001b[1;32m    571\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunked\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msupports_chunked_reads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_chunked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecode_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/urllib3/response.py\u001b[0m in \u001b[0;36mread_chunked\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_chunk_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_left\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/urllib3/response.py\u001b[0m in \u001b[0;36m_update_chunk_length\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    692\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_left\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 694\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    695\u001b[0m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb\";\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    696\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1239\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1240\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1241\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1242\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1097\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1099\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1100\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for index, row in df_input.iterrows():\n",
    "    url_id = row[\"URL_ID\"]\n",
    "    url = row[\"URL\"]\n",
    "    article_title, article_text = extract_article_content(url)\n",
    "    \n",
    "    if article_title and article_text:\n",
    "    # Save the article content in a text file\n",
    "        with open(f\"{url_id}.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(article_title + \"\\n\")\n",
    "            file.write(article_text)\n",
    "#don't run just to check that we are getting the content in each text file or not(we are executing same in below)\n",
    "#after few creating text just restart the kernel or you can see the 113 text files (3 files are 404 not found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2aae12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e444673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Loading Stop Words \n",
    "#loaded the stopwords from folder\n",
    "def load_stop_words_from_folder(folder_path, encodings=['utf-8', 'latin-1']):\n",
    "    stop_words = set()\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        for encoding in encodings:\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding=encoding) as file:\n",
    "                    words = file.read().splitlines()\n",
    "                    stop_words.update(words)\n",
    "                break  # Stop if successful\n",
    "            except UnicodeDecodeError:\n",
    "                continue  # Try the next encoding if there's an error\n",
    "    return stop_words\n",
    "stop_words = load_stop_words_from_folder(\"StopWords\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "497289bb",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'KENDRICK',\n",
       " 'MANA',\n",
       " 'MAES',\n",
       " 'SKILES',\n",
       " 'KELLIE',\n",
       " 'MERRIWEATHER',\n",
       " 'JOANN',\n",
       " 'BOHN',\n",
       " 'LOVING',\n",
       " 'GRANTHAM',\n",
       " 'KRONE | Denmark ',\n",
       " 'MCCOMAS',\n",
       " 'KURTZ',\n",
       " 'JACOBS',\n",
       " 'WHIPPLE',\n",
       " 'think',\n",
       " 'JACOBSON',\n",
       " 'DOOLITTLE',\n",
       " 'RUNYAN',\n",
       " 'says',\n",
       " 'BRETON',\n",
       " 'JAMIESON',\n",
       " 'STRAUB',\n",
       " 'gets',\n",
       " 'other',\n",
       " 'GIDGET',\n",
       " 'MAURICE',\n",
       " 'ARDOIN',\n",
       " 'ALEMAN',\n",
       " 'SWINK',\n",
       " 'GITA',\n",
       " 'WISCONSIN',\n",
       " 'TAMEIKA',\n",
       " 'FLORIN | Aruba ',\n",
       " 'FRINK',\n",
       " 'what',\n",
       " 'BURKE',\n",
       " 'HIMES',\n",
       " 'PETE',\n",
       " 'TREVINO',\n",
       " 'SHANE',\n",
       " 'GRAVES',\n",
       " 'SERRATO',\n",
       " 'SZABO',\n",
       " 'RUDOLPH',\n",
       " 'BARGER',\n",
       " 'BELGIUM',\n",
       " 'HARE',\n",
       " 'LALIBERTE',\n",
       " 'JAYME',\n",
       " 'VANPELT',\n",
       " 'ALISIA',\n",
       " 'PRINCE',\n",
       " 'PERRINE',\n",
       " 'LUKER',\n",
       " 'WHITWORTH',\n",
       " 'BENNETT',\n",
       " 'ROBERTO',\n",
       " 'ELYSE',\n",
       " 'LYLA',\n",
       " 'FITZ',\n",
       " 'FORTIER',\n",
       " 'POE',\n",
       " 'POND',\n",
       " 'DENNA',\n",
       " 'CUNDIFF',\n",
       " 'MILAGRO',\n",
       " 'KIMURA',\n",
       " 'WOODBURY',\n",
       " 'ROSENTHAL',\n",
       " 'SHAD',\n",
       " 'LYNETTE',\n",
       " 'regarding',\n",
       " 'GILBERTSON',\n",
       " 'MARAGARET',\n",
       " 'BUNN',\n",
       " 'HARKINS',\n",
       " 'NELDA',\n",
       " 'JEFFREY',\n",
       " 'FINE',\n",
       " 'HERSHEL',\n",
       " 'DALE',\n",
       " 'RESENDEZ',\n",
       " 'SOO',\n",
       " 'ELI',\n",
       " 'nearly',\n",
       " 'SIEBERT',\n",
       " 'NIKIA',\n",
       " 'LEVITT',\n",
       " 'NANCEE',\n",
       " 'DOWDY',\n",
       " 'SUSAN',\n",
       " 'DARLENE',\n",
       " 'PIPPIN',\n",
       " 'HOLLIFIELD',\n",
       " 'CRISTIE',\n",
       " 'CALLAHAN',\n",
       " 'CARMINE',\n",
       " 'LAKEISHA',\n",
       " 'BONNEY',\n",
       " 'ALESHA',\n",
       " 'GILMORE',\n",
       " 'BURBANK',\n",
       " 'b',\n",
       " 'BABETTE',\n",
       " 'DAWES',\n",
       " 'JOAN',\n",
       " 'GISELA',\n",
       " 'ADOLFO',\n",
       " 'GROFF',\n",
       " 'DELOISE',\n",
       " 'SURBER',\n",
       " 'GIUSEPPINA',\n",
       " 'MOSS',\n",
       " 'DIONNA',\n",
       " 'FRANSISCA',\n",
       " 'clearly',\n",
       " 'NINFA',\n",
       " 'SHARYN',\n",
       " 'SHAVER',\n",
       " 'YOURSELF',\n",
       " 'MCDOUGALL',\n",
       " 'TOOLE',\n",
       " 'BIGHAM',\n",
       " 'FREDERICKS',\n",
       " 'FARWELL',\n",
       " 'EUGENE',\n",
       " 'JOLLY',\n",
       " 'MARLA',\n",
       " 'SAMANTHA',\n",
       " 'VANNESA',\n",
       " 'EARLENE',\n",
       " 'CARPENTER',\n",
       " 'MCGEHEE',\n",
       " 'BLONDELL',\n",
       " 'BEEN',\n",
       " 'VALERIA',\n",
       " 'BUNCH',\n",
       " 'BANNER',\n",
       " 'ARELLANO',\n",
       " 'ZAVALA',\n",
       " 'GILLETTE',\n",
       " 'VINEYARD',\n",
       " 'WILLENA',\n",
       " 'WENDOLYN',\n",
       " 'TRUESDALE',\n",
       " 'YEAR',\n",
       " 'SAVOIE',\n",
       " 'merely',\n",
       " 'HALLEY',\n",
       " 'BRISTOL',\n",
       " 'WALTRAUD',\n",
       " 'VERDIE',\n",
       " 'MOUA',\n",
       " 'AVA',\n",
       " 'ROSANN',\n",
       " 'MARIS',\n",
       " 'HAVEN',\n",
       " 'MARGIE',\n",
       " 'JARRETT',\n",
       " 'KIESHA',\n",
       " 'FROM',\n",
       " 'TUBBS',\n",
       " 'STEEN',\n",
       " 'ASHWORTH',\n",
       " 'RITZ',\n",
       " 'MARLENE',\n",
       " 'CAMILLE',\n",
       " 'PLOURDE',\n",
       " 'SEWELL',\n",
       " 'GIOVANNI',\n",
       " 'YOHO',\n",
       " 'DRISKELL',\n",
       " 'EWING',\n",
       " 'ADDINGTON',\n",
       " 'PICHARDO',\n",
       " 'LEMMONS',\n",
       " 'DICARLO',\n",
       " 'CATO',\n",
       " 'FRANCOISE',\n",
       " 'LALA',\n",
       " 'TONY',\n",
       " 'YOUNGBLOOD',\n",
       " 'PEREZ',\n",
       " 'GRABER',\n",
       " 'SYDNEY',\n",
       " 'MERCEDEZ',\n",
       " 'ELLIE',\n",
       " 'EASON',\n",
       " 'APONTE',\n",
       " 'DANIAL',\n",
       " 'MARILU',\n",
       " 'GILMA',\n",
       " 'BOONE',\n",
       " 'JOLYN',\n",
       " 'FATIMAH',\n",
       " 'not',\n",
       " 'ULMER',\n",
       " 'CRUTCHER',\n",
       " 'KLOTZ',\n",
       " 'NANCI',\n",
       " 'KIMIKO',\n",
       " 'BANKER',\n",
       " 'whatever',\n",
       " 'HAZARD',\n",
       " 'CHERLYN',\n",
       " 'HUMES',\n",
       " 'SUNDAY',\n",
       " 'GILLAM',\n",
       " 'RAZO',\n",
       " 'KERRI',\n",
       " 'UNDERWOOD',\n",
       " 'NETHERLANDS',\n",
       " 'WINCHELL',\n",
       " 'HADLEY',\n",
       " 'ESTEFANA',\n",
       " 'RICHARDSON',\n",
       " 'FIKE',\n",
       " 'VALDA',\n",
       " 'CLARY',\n",
       " 'GERRI',\n",
       " 'EASTERLING',\n",
       " 'ANASTASIA',\n",
       " 'DOVER',\n",
       " 'already',\n",
       " 'MAGDA',\n",
       " 'DARREN',\n",
       " 'PILAR',\n",
       " 'FONTENOT',\n",
       " 'SHAVON',\n",
       " 'MENDY',\n",
       " 'EACH',\n",
       " 'ESTRADA',\n",
       " 'NOLL',\n",
       " 'ASHLEY',\n",
       " 'ALDRICH',\n",
       " 'LOWELL',\n",
       " 'ERNA',\n",
       " 'FROST',\n",
       " 'another',\n",
       " 'SAM',\n",
       " 'STILTNER',\n",
       " 'FERRIS',\n",
       " 'PHARR',\n",
       " 'FRANKE',\n",
       " 'GRANT',\n",
       " 'TAFT',\n",
       " 'PLEASANT',\n",
       " 'DEHAVEN',\n",
       " 'SETTLES',\n",
       " 'MELANIA',\n",
       " 'GENEVIEVE',\n",
       " 'ALLRED',\n",
       " 'HULBERT',\n",
       " 'CARRASQUILLO',\n",
       " 'IN',\n",
       " 'BRUNO',\n",
       " 'REANNA',\n",
       " 'DORTHEY',\n",
       " 'NEW SHEQEL  | Israel ',\n",
       " 'VANHORN',\n",
       " 'CEDRIC',\n",
       " 'DOSS',\n",
       " 'MOOREHEAD',\n",
       " 'ROMAINE',\n",
       " 'COATS',\n",
       " 'CHARLOTTE',\n",
       " 'KLAUS',\n",
       " 'DOUCETTE',\n",
       " 'DEMERS',\n",
       " 'PEEL',\n",
       " 'HOMER',\n",
       " 'MARTINA',\n",
       " 'WANITA',\n",
       " 'BRINKLEY',\n",
       " 'GEIGER',\n",
       " 'ARTIE',\n",
       " 'BUSS',\n",
       " 'HARRIET',\n",
       " 'HITCHCOCK',\n",
       " 'vs',\n",
       " 'KISTLER',\n",
       " 'GEORGEANNA',\n",
       " 'BRANDIE',\n",
       " 'STOREY',\n",
       " 'SAUCEDO',\n",
       " 'SESSOMS',\n",
       " 'JIMERSON',\n",
       " 'BURWELL',\n",
       " 'KEILA',\n",
       " 'TAKAKO',\n",
       " 'WILDE',\n",
       " 'ANGILA',\n",
       " 'STUART',\n",
       " 'MARQUITTA',\n",
       " 'SALAS',\n",
       " 'MACHELLE',\n",
       " 'CLEMENTE',\n",
       " 'ROBBYN',\n",
       " 'BROTHERTON',\n",
       " 'JABLONSKI',\n",
       " 'GOTTLIEB',\n",
       " 'ELOUISE',\n",
       " 'MARKS',\n",
       " 'PARTAIN',\n",
       " 'MARCOS',\n",
       " 'SOLEDAD',\n",
       " 'VANBUSKIRK',\n",
       " 'RUFFIN',\n",
       " 'PAREDES',\n",
       " 'COLLEY',\n",
       " 'HANLEY',\n",
       " 'BOUTTE',\n",
       " 'GARAY',\n",
       " 'MICHELS',\n",
       " 'MASON',\n",
       " 'obviously',\n",
       " 'BRACEY',\n",
       " 'BALL',\n",
       " 'RHONDA',\n",
       " 'looking',\n",
       " 'DINKINS',\n",
       " 'HILARY',\n",
       " 'GOOD',\n",
       " 'HORTENSE',\n",
       " 'GUSMAN',\n",
       " 'CHONG',\n",
       " 'just',\n",
       " 'GRIFFIN',\n",
       " 'HOFFER',\n",
       " 'NICOLETTE',\n",
       " 'ANDRIA',\n",
       " 'STPIERRE',\n",
       " 'RUPPERT',\n",
       " 'DESANTIS',\n",
       " 'URBINA',\n",
       " 'LORILEE',\n",
       " 'PHELPS',\n",
       " 'MESSENGER',\n",
       " 'VANDERPOOL',\n",
       " 'KRYSTYNA',\n",
       " 'DANTE',\n",
       " 'ALESIA',\n",
       " 'MONROE',\n",
       " 'JACQUELYN',\n",
       " 'MISS',\n",
       " 'XVII',\n",
       " 'BULL',\n",
       " 'SHEPHERD',\n",
       " 'TULLY',\n",
       " 'LAWSON',\n",
       " 'ANTONIO',\n",
       " 'DEVONA',\n",
       " 'FRIEDMAN',\n",
       " 'FRIAS',\n",
       " 'JANINA',\n",
       " 'HOWZE',\n",
       " 'AHMAD',\n",
       " 'BUTCHER',\n",
       " 'MCCAFFERTY',\n",
       " 'EHRLICH',\n",
       " 'RAYMOND',\n",
       " 'VILLASENOR',\n",
       " 'UPTON',\n",
       " 'VOIGT',\n",
       " 'BOTTOMS',\n",
       " 'HORVATH',\n",
       " 'MINERVA',\n",
       " 'ASHBURN',\n",
       " 'MELIA',\n",
       " 'REGENIA',\n",
       " 'KAHN',\n",
       " 'KEEGAN',\n",
       " 'JOHNNIE',\n",
       " 'VINCENZO',\n",
       " 'TATUM',\n",
       " 'HAYDEE',\n",
       " 'TAWANDA',\n",
       " 'ILDA',\n",
       " 'STAPLETON',\n",
       " 'ALLINE',\n",
       " 'EDISON',\n",
       " 'VANHOUTEN',\n",
       " 'SHONNA',\n",
       " 'ABBEY',\n",
       " 'REINKE',\n",
       " 'MICHELLE',\n",
       " 'LEMPIRA | Honduras ',\n",
       " 'TARVER',\n",
       " 'MARICA',\n",
       " 'COLLENE',\n",
       " 'BLANCH',\n",
       " 'VANDYKE',\n",
       " 'RASH',\n",
       " 'LYNCH',\n",
       " 'NISSEN',\n",
       " 'DAVIDA',\n",
       " 'SACHS',\n",
       " 'JERRICA',\n",
       " 'VESTA',\n",
       " 'JERI',\n",
       " 'RASMUSSEN',\n",
       " 'MIZE',\n",
       " 'OUELLETTE',\n",
       " 'PAINE',\n",
       " 'PLUMMER',\n",
       " 'FRICK',\n",
       " 'HORNBACK',\n",
       " 'BATES',\n",
       " 'GUSSIE',\n",
       " 'HOMAN',\n",
       " 'ESPOSITO',\n",
       " 'ask',\n",
       " 'AILEEN',\n",
       " 'MISHA',\n",
       " 'DYKSTRA',\n",
       " 'SPAULDING',\n",
       " 'TROTT',\n",
       " 'ARREDONDO',\n",
       " 'SPARKLE',\n",
       " 'SAMUELS',\n",
       " 'RAINEY',\n",
       " 'ALBINA',\n",
       " 'TENISHA',\n",
       " 'HILARIA',\n",
       " 'MODESTO',\n",
       " 'REBER',\n",
       " 'BERNEICE',\n",
       " 'LASHON',\n",
       " 'CLOTILDE',\n",
       " 'SEVENTH',\n",
       " 'FUNDERBURK',\n",
       " 'CEDRICK',\n",
       " 'us',\n",
       " 'WENDY',\n",
       " 'HARR',\n",
       " 'CADENA',\n",
       " 'AGUAYO',\n",
       " 'RUDOLF',\n",
       " 'VICKI',\n",
       " 'ROD',\n",
       " 'LEIGHA',\n",
       " 'JAMEE',\n",
       " 'ALVES',\n",
       " 'SHELLIE',\n",
       " 'ARACELIS',\n",
       " 'JANELLE',\n",
       " 'RENATO',\n",
       " 'BRIANA',\n",
       " 'will',\n",
       " 'STRUCK',\n",
       " 'CLAPP',\n",
       " 'SWANK',\n",
       " 'THROWER',\n",
       " 'AMI',\n",
       " 'BECKER',\n",
       " 'JAIMIE',\n",
       " 'SAIN',\n",
       " 'MENDOZA',\n",
       " 'OLETA',\n",
       " 'JANEEN',\n",
       " 'HAGGARD',\n",
       " 'HARTSELL',\n",
       " 'FRIDAY',\n",
       " 'u',\n",
       " 'HARGETT',\n",
       " 'ABOUT',\n",
       " 'RONNI',\n",
       " 'LUCI',\n",
       " 'LATICIA',\n",
       " 'HALSTEAD',\n",
       " 'CARLEE',\n",
       " 'OSULLIVAN',\n",
       " 'SULLINS',\n",
       " 'HAGGERTY',\n",
       " 'LOWREY',\n",
       " 'SCHOTT',\n",
       " 'BOBBYE',\n",
       " 'STALLINGS',\n",
       " 'WESTCOTT',\n",
       " 'LAVERGNE',\n",
       " 'FARKAS',\n",
       " 'NANNETTE',\n",
       " 'SHARITA',\n",
       " 'ARNETTE',\n",
       " 'MERI',\n",
       " 'k',\n",
       " 'DETROIT',\n",
       " 'SORENSEN',\n",
       " 'BERNAL',\n",
       " 'MANOR',\n",
       " 'MALOY',\n",
       " 'GAYNELLE',\n",
       " 'DEVORAH',\n",
       " 'FINNEGAN',\n",
       " 'MELBA',\n",
       " 'WARRICK',\n",
       " 'TILLER',\n",
       " 'TILDA',\n",
       " 'WOODS',\n",
       " 'seriously',\n",
       " 'SEDILLO',\n",
       " 'SWISHER',\n",
       " 'VARGA',\n",
       " 'BRITTNEY',\n",
       " 'one',\n",
       " 'SHAUNTA',\n",
       " 'OLA',\n",
       " 'GLEN',\n",
       " 'BEY',\n",
       " 'WEEKS',\n",
       " 'ISAACSON',\n",
       " 'CASSELL',\n",
       " 'MARKLE',\n",
       " 'KHADIJAH',\n",
       " 'HOBERT',\n",
       " 'HOLLIE',\n",
       " 'KRISTINE',\n",
       " 'are',\n",
       " 'SANSONE',\n",
       " 'MCPHERSON',\n",
       " 'ROSELLA',\n",
       " 'DEANNA',\n",
       " 'JASMIN',\n",
       " 'ZORA',\n",
       " 'DARDAR',\n",
       " 'MOHLER',\n",
       " 'DONITA',\n",
       " 'ALEXIS',\n",
       " 'keeps',\n",
       " 'DION',\n",
       " 'MIMS',\n",
       " 'SHIREY',\n",
       " 'AVERY',\n",
       " 'MUMBAI',\n",
       " 'CADY',\n",
       " 'TOMS',\n",
       " 'LANGLOIS',\n",
       " 'consequently',\n",
       " 'CORSON',\n",
       " 'SILVEIRA',\n",
       " 'LARK',\n",
       " 'YUK',\n",
       " 'SOUZA',\n",
       " 'HILDA',\n",
       " 'KONVERTIBILNA MARKA  | Bosnia-Herzegovina ',\n",
       " 'MCCLURE',\n",
       " 'VALENE',\n",
       " 'that',\n",
       " 'SHINN',\n",
       " 'JENAE',\n",
       " 'FERREIRA',\n",
       " 'ARRINGTON',\n",
       " 'LLEWELLYN',\n",
       " 'SHILOH',\n",
       " 'CAROLINA',\n",
       " 'OUTLAW',\n",
       " 'KATELIN',\n",
       " 'TOSHIKO',\n",
       " 'PERES',\n",
       " 'TAREN',\n",
       " 'RAINS',\n",
       " 'NORAH',\n",
       " 'DENISON',\n",
       " 'GLICK',\n",
       " 'EVERSOLE',\n",
       " 'ROME',\n",
       " 'KAVANAGH',\n",
       " 'then',\n",
       " 'MOULTRIE',\n",
       " 'THRASH',\n",
       " 'JESTINE',\n",
       " 'LERNER',\n",
       " 'LAZARUS',\n",
       " 'BUIE',\n",
       " 'LORA',\n",
       " 'VERNA',\n",
       " 'CARMELA',\n",
       " 'PHOEBE',\n",
       " 'mainly',\n",
       " 'CANDIE',\n",
       " 'ELVIA',\n",
       " 'JUDGE',\n",
       " 'IVA',\n",
       " 'DOMINGUEZ',\n",
       " 'MADONNA',\n",
       " 'ALFREDA',\n",
       " 'WORDEN',\n",
       " 'EFFIE',\n",
       " 'HANLON',\n",
       " 'ROWELL',\n",
       " 'MINNIE',\n",
       " 'MOONEY',\n",
       " 'HEFFNER',\n",
       " 'ALISSA',\n",
       " 'KATHLINE',\n",
       " \"you've\",\n",
       " 'RICHMOND',\n",
       " 'WORTHEN',\n",
       " 'SEA',\n",
       " 'ODELL',\n",
       " 'elsewhere',\n",
       " 'SNAPP',\n",
       " 'LAPIERRE',\n",
       " 'ARTEAGA',\n",
       " 'HYACINTH',\n",
       " 'TESSA',\n",
       " 'DUPONT',\n",
       " 'MUI',\n",
       " 'THEMSELVES',\n",
       " 'TOMAS',\n",
       " 'DELORIS',\n",
       " 'GLASS',\n",
       " 'LAYNE',\n",
       " 'ALBERTINA',\n",
       " 'GREATHOUSE',\n",
       " 'SLEDGE',\n",
       " 'BIANCO',\n",
       " 'ELENI',\n",
       " 'DOCKERY',\n",
       " 'ALEXA',\n",
       " 'AZUCENA',\n",
       " 'CLOUSE',\n",
       " 'TINISHA',\n",
       " 'OSBORN',\n",
       " 'whereas',\n",
       " 'CONNIE',\n",
       " 'KYUNG',\n",
       " 'RADCLIFF',\n",
       " 'TEJEDA',\n",
       " 'VERLIE',\n",
       " 'RENEE',\n",
       " 'DON',\n",
       " 'GERLACH',\n",
       " 'SOSA',\n",
       " 'NESTOR',\n",
       " 'LITAS | Lithuania ',\n",
       " 'LAFOUNTAIN',\n",
       " 'MARYLN',\n",
       " 'ALBERTHA',\n",
       " 'AUCOIN',\n",
       " 'DOMITILA',\n",
       " 'FREY',\n",
       " 'SCULLY',\n",
       " 'SEGAL',\n",
       " 'GRUNDY',\n",
       " 'OCAMPO',\n",
       " 'GUAY',\n",
       " 'SHELLI',\n",
       " 'GRIDER',\n",
       " 'LYNDSEY',\n",
       " 'POK',\n",
       " 'JERRY',\n",
       " 'TANNER',\n",
       " 'CLARKSON',\n",
       " 'beside',\n",
       " 'CARISA',\n",
       " 'which',\n",
       " 'MARTINEAU',\n",
       " 'MAGALI',\n",
       " 'despite',\n",
       " 'DUQUETTE',\n",
       " 'HAMMON',\n",
       " 'MCNAMARA',\n",
       " 'JUNKO',\n",
       " 'MCLEMORE',\n",
       " 'TATE',\n",
       " 'FRANZ',\n",
       " 'CEBALLOS',\n",
       " 'LABBE',\n",
       " 'WISDOM',\n",
       " 'KRONA | Sweden ',\n",
       " 'ADRIENE',\n",
       " 'DELATORRE',\n",
       " 'JAMES',\n",
       " 'RIVERO',\n",
       " 'OGLE',\n",
       " 'DEROSA',\n",
       " 'ALFREDO',\n",
       " 'GONZALEZ',\n",
       " 'LATTIMORE',\n",
       " 'VALLADARES',\n",
       " 'WELBORN',\n",
       " 'RED',\n",
       " 'CARLTON',\n",
       " 'BORGES',\n",
       " 'ROBBINS',\n",
       " 'KATE',\n",
       " 'PATTIE',\n",
       " 'PACKER',\n",
       " 'BOYKIN',\n",
       " 'GILMER',\n",
       " 'HOOKER',\n",
       " 'BOUDREAU',\n",
       " 'PALERMO',\n",
       " 'LORIE',\n",
       " 'BUSBY',\n",
       " 'LISBETH',\n",
       " 'BOLLING',\n",
       " 'KUHNS',\n",
       " 'BARAN',\n",
       " 'GAIL',\n",
       " 'CARLO',\n",
       " 'FLORIA',\n",
       " 'WARDEN',\n",
       " 'POINDEXTER',\n",
       " 'ALBRITTON',\n",
       " 'DANIA',\n",
       " 'GREGG',\n",
       " 'TENNESSEE',\n",
       " 'WALLING',\n",
       " 'TEN',\n",
       " 'SHAWNDA',\n",
       " 'FABER',\n",
       " 'GAYLE',\n",
       " 'RAFAELA',\n",
       " 'ISADORA',\n",
       " 'SINK',\n",
       " 'SADYE',\n",
       " 'EDRIS',\n",
       " 'PEYTON',\n",
       " 'BLISS',\n",
       " 'VOGEL',\n",
       " 'BUGGS',\n",
       " 'BOULDIN',\n",
       " 'TYNDALL',\n",
       " 'HIEN',\n",
       " 'HOSTETTER',\n",
       " 'LOMAS',\n",
       " 'HEWLETT',\n",
       " 'JOETTA',\n",
       " 'need',\n",
       " 'RHYNE',\n",
       " 'GUNTER',\n",
       " 'MOORMAN',\n",
       " 'REGGIE',\n",
       " 'DEBBY',\n",
       " 'LI  |  http://en.wikipedia.org/wiki/List_of_most_common_surnames#China',\n",
       " 'PAK',\n",
       " 'RONALD',\n",
       " 'WEN',\n",
       " 'PARTICIA',\n",
       " 'NEOMA',\n",
       " 'RICARDO',\n",
       " 'HOULE',\n",
       " 'ELENORA',\n",
       " 'TAMEKIA',\n",
       " 'FLOR',\n",
       " 'CLINT',\n",
       " 'GAYE',\n",
       " 'MOREHOUSE',\n",
       " 'PACHECO',\n",
       " 'LORETA',\n",
       " 'CHU',\n",
       " 'LAURETTA',\n",
       " 'MICHIGAN',\n",
       " 'CHILE',\n",
       " 'too',\n",
       " 'SHELA',\n",
       " 'LEFLORE',\n",
       " 'ELDON',\n",
       " 'STEELE',\n",
       " 'JUTTA',\n",
       " 'EMBRY',\n",
       " 'HINOJOSA',\n",
       " 'ROUNDS',\n",
       " 'DUGGER',\n",
       " 'JOSE',\n",
       " 'SCHULZ',\n",
       " 'PEARLINE',\n",
       " 'SHANEKA',\n",
       " 'RUGGLES',\n",
       " 'sure',\n",
       " 'WAGGONER',\n",
       " 'VAIL',\n",
       " 'BLODGETT',\n",
       " 'CHARLOTT',\n",
       " 'GISELE',\n",
       " 'LORNA',\n",
       " 'ROLANDA',\n",
       " 'WILHELM',\n",
       " 'DESTEFANO',\n",
       " 'CATHCART',\n",
       " 'ALVIN',\n",
       " 'RACHELL',\n",
       " 'JOANNA',\n",
       " 'OUGUIYA  | Mauritania ',\n",
       " 'REDDY',\n",
       " 'HOSEY',\n",
       " 'SARAN',\n",
       " 'ALLYSON',\n",
       " 'FREDERICK',\n",
       " 'SHOEMAKER',\n",
       " 'MANSON',\n",
       " 'CORNWELL',\n",
       " 'EGBERT',\n",
       " 'DRESSLER',\n",
       " 'HAKE',\n",
       " 'RANDY',\n",
       " 'YAHAIRA',\n",
       " 'LETITIA',\n",
       " 'ELISE',\n",
       " 'YOKO',\n",
       " 'JONES',\n",
       " 'PITCHER',\n",
       " 'FEE',\n",
       " 'MENDES',\n",
       " 'ANSON',\n",
       " 'COPELAND',\n",
       " 'TALLEY',\n",
       " 'FOLTZ',\n",
       " 'SAUL',\n",
       " 'JACQUELINE',\n",
       " 'SLYVIA',\n",
       " 'TOKYO',\n",
       " 'ANTHONY',\n",
       " 'MCFARLANE',\n",
       " 'TYREE',\n",
       " 'CYNDI',\n",
       " 'CATE',\n",
       " 'LO',\n",
       " 'LAMP',\n",
       " 'BREEDLOVE',\n",
       " 'ADRIA',\n",
       " 'CEOLA',\n",
       " 'CLANCY',\n",
       " 'NAM',\n",
       " 'GILMAN',\n",
       " 'TRENTON',\n",
       " 'BUCKNER',\n",
       " 'ROGERS',\n",
       " 'RUBYE',\n",
       " 'currently',\n",
       " 'KELVIN',\n",
       " 'BOTT',\n",
       " 'MERRIE',\n",
       " 'REX',\n",
       " 'RANDELL',\n",
       " 'GREGORIO',\n",
       " 'MOREIRA',\n",
       " 'PORTERFIELD',\n",
       " 'THREE',\n",
       " 'CORETTA',\n",
       " 'COVER',\n",
       " 'TURNAGE',\n",
       " 'BOOKER',\n",
       " 'CASILLAS',\n",
       " 'MARYLYNN',\n",
       " 'MAMMIE',\n",
       " 'SOPHIA',\n",
       " 'PARISI',\n",
       " 'AFRICA',\n",
       " 'STEFANIE',\n",
       " 'HERMINA',\n",
       " 'RENMINBI  | China ',\n",
       " 'DEWS',\n",
       " 'KOURTNEY',\n",
       " 'ALVERA',\n",
       " 'ANDRES',\n",
       " 'FELICIDAD',\n",
       " 'MARS',\n",
       " 'SHANICE',\n",
       " 'MARCIA',\n",
       " 'ANNAMARIA',\n",
       " 'WHORTON',\n",
       " 'TOBIE',\n",
       " 'FOLEY',\n",
       " 'DIAS',\n",
       " 'LUCY',\n",
       " 'OLIVIER',\n",
       " 'BOLES',\n",
       " 'CRISS',\n",
       " 'FLOOD',\n",
       " 'LEANORA',\n",
       " 'CHEUNG',\n",
       " 'HANSEL',\n",
       " 'ORTEGA',\n",
       " 'SPILLER',\n",
       " 'GARNET',\n",
       " 'DOUGHTY',\n",
       " 'KELLAM',\n",
       " 'GUERIN',\n",
       " 'DIXIE',\n",
       " 'VANESSA',\n",
       " 'NATHANIAL',\n",
       " 'QUILLEN',\n",
       " 'LAWS',\n",
       " 'HOYLE',\n",
       " 'PEABODY',\n",
       " 'WAGES',\n",
       " 'JAMAR',\n",
       " 'STEADMAN',\n",
       " 'RHETT',\n",
       " 'ERSKINE',\n",
       " 'PAGAN',\n",
       " 'MARKER',\n",
       " 'wonder',\n",
       " 'EZEKIEL',\n",
       " 'OTOOLE',\n",
       " 'WENTWORTH',\n",
       " 'CHOI',\n",
       " 'ACTON',\n",
       " 'far',\n",
       " 'ISAAC',\n",
       " 'ATKINS',\n",
       " 'SHASTA',\n",
       " 'VENNIE',\n",
       " 'DELEON',\n",
       " 'APPLEGATE',\n",
       " 'LEDEZMA',\n",
       " 'COURTNEY',\n",
       " 'COLMAN',\n",
       " 'STCLAIR',\n",
       " 'SONG',\n",
       " 'HASSELL',\n",
       " 'FLOY',\n",
       " 'FUDGE',\n",
       " 'INGE',\n",
       " 'NERY',\n",
       " 'FALGOUST',\n",
       " 'MADRIGAL',\n",
       " 'ANGELA',\n",
       " 'RHOADES',\n",
       " 'IRVINE',\n",
       " 'ARDEN',\n",
       " 'APODACA',\n",
       " 'PALMIERI',\n",
       " 'SHANIKA',\n",
       " 'HUFF',\n",
       " 'LAIL',\n",
       " 'CORRIE',\n",
       " 'HOLLYWOOD',\n",
       " 'JANES',\n",
       " 'COLLIN',\n",
       " 'LUMPKIN',\n",
       " 'PINKSTON',\n",
       " 'FRAN',\n",
       " 'JENNIFFER',\n",
       " 'ERICKA',\n",
       " 'IVERSON',\n",
       " 'DINSMORE',\n",
       " 'PHILLIPS',\n",
       " 'MARYROSE',\n",
       " 'CLYDE',\n",
       " 'LIBBY',\n",
       " 'TANESHA',\n",
       " 'ROBICHAUX',\n",
       " 'yours',\n",
       " 'SCOTTY',\n",
       " 'JASSO',\n",
       " 'WALTER',\n",
       " 'downwards',\n",
       " 'BAUMGARTNER',\n",
       " 'SHOWALTER',\n",
       " 'FELTON',\n",
       " 'FINLEY',\n",
       " 'SWEARINGEN',\n",
       " 'DIANA',\n",
       " 'HAWAII',\n",
       " 'MORRISSETTE',\n",
       " 'KATHEY',\n",
       " 'LACEY',\n",
       " 'JERRI',\n",
       " 'FORSYTHE',\n",
       " 'KRISTEEN',\n",
       " 'x',\n",
       " 'BEGGS',\n",
       " 'EBERT',\n",
       " 'DRIVER',\n",
       " 'BURTON',\n",
       " 'ELISSA',\n",
       " 'FIDELIA',\n",
       " 'BOWE',\n",
       " 'CHANEL',\n",
       " 'FIDELA',\n",
       " 'YURIKO',\n",
       " 'BATTLES',\n",
       " 'BENTON',\n",
       " 'MEMPHIS',\n",
       " 'GILLEY',\n",
       " 'IRIZARRY',\n",
       " 'FORSYTH',\n",
       " 'MERRICK',\n",
       " 'WACHTER',\n",
       " 'CHESSER',\n",
       " 'TREJO',\n",
       " 'VELMA',\n",
       " 'CUSHMAN',\n",
       " 'ALTAGRACIA',\n",
       " 'appear',\n",
       " 'CARLYN',\n",
       " 'WILLS',\n",
       " 'HAGAN',\n",
       " 'BERMAN',\n",
       " 'MOLLER',\n",
       " 'MACPHERSON',\n",
       " 'ALDRIDGE',\n",
       " 'HARBISON',\n",
       " 'CORDELL',\n",
       " 'ZACHARY',\n",
       " ...}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words   \n",
    "#to check the stop words from (7)text file is store in this stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bbd6fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "311945c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Text Cleaning \n",
    "clean = []\n",
    "def clean_text(text, stop_words):\n",
    "    # Remove punctuation\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    text = text.translate(translator)\n",
    "    \n",
    "    # Convert text to lowercase and split into words\n",
    "    words = text.lower().split()\n",
    "    \n",
    "    # Exclude stop words\n",
    "    cleaned_words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # Join cleaned words back to a sentence\n",
    "    cleaned_text = \" \".join(cleaned_words)\n",
    "    \n",
    "    return cleaned_text  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a902a92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02835770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Sentiment Analysis and Derived Variables\n",
    "\n",
    "# Load the positive and negative words from the MasterDictionary folder\n",
    "master_dictionary_folder = \"MasterDictionary\"\n",
    "positive_words_file = \"positive-words.txt\"\n",
    "negative_words_file = \"negative-words.txt\"\n",
    "positive_words_path = os.path.join(master_dictionary_folder, positive_words_file)\n",
    "negative_words_path = os.path.join(master_dictionary_folder, negative_words_file)\n",
    "positive_words = load_stop_words(positive_words_path)\n",
    "negative_words = load_stop_words(negative_words_path)\n",
    "\n",
    "# Define a function to calculate the positive and negative scores\n",
    "\n",
    "def calculate_positive_negative_scores(cleaned_text, positive_words, negative_words):\n",
    "    positive_score = sum(1 for word in cleaned_text.split() if word in positive_words)\n",
    "    negative_score = sum(1 for word in cleaned_text.split() if word in negative_words)\n",
    "    return positive_score, negative_score\n",
    "\n",
    "# Define a function to calculate the polarity score and subjectivity score\n",
    "def calculate_polarity_subjectivity_scores(positive_score, negative_score, total_words_after_cleaning):\n",
    "    polarity_score = (positive_score - negative_score) / (positive_score + negative_score + 0.000001)\n",
    "    subjectivity_score = (positive_score + negative_score) / (total_words_after_cleaning + 0.000001)\n",
    "    return polarity_score, subjectivity_score\n",
    "\n",
    "# Define a function to calculate the average sentence length\n",
    "def calculate_average_sentence_length(cleaned_text):\n",
    "    sentences = nltk.sent_tokenize(cleaned_text)\n",
    "    total_words = sum(len(word_tokenize(sentence)) for sentence in sentences)\n",
    "    total_sentences = len(sentences)\n",
    "    return total_words / total_sentences\n",
    "\n",
    "# Define a function to calculate the percentage of complex words\n",
    "def calculate_complex_word_percentage(cleaned_text):\n",
    "    words = word_tokenize(cleaned_text)\n",
    "    complex_words = [word for word in words if count_syllables(word) > 2]\n",
    "    return len(complex_words) / len(words)\n",
    "\n",
    "# Define a function to calculate the Fog Index\n",
    "def calculate_fog_index(average_sentence_length, complex_word_percentage):\n",
    "    return 0.4 * (average_sentence_length + complex_word_percentage)\n",
    "\n",
    "# Define a function to calculate the average number of words per sentence\n",
    "def calculate_average_words_per_sentence(cleaned_text):\n",
    "    sentences = nltk.sent_tokenize(cleaned_text)\n",
    "    total_words = sum(len(word_tokenize(sentence)) for sentence in sentences)\n",
    "    total_sentences = len(sentences)\n",
    "    return total_words / total_sentences\n",
    "\n",
    "# Define a function to count the number of syllables in a word\n",
    "def count_syllables(word):\n",
    "    d = cmudict.dict()\n",
    "    if word.lower() in d:\n",
    "        return [len(list(y for y in x if y[-1].isdigit())) for x in d[word.lower()]][0]\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Define a function to count the number of personal pronouns\n",
    "def count_personal_pronouns(cleaned_text):\n",
    "    pronoun_regex = r'\\b(?:I|we|my|ours|us)\\b'\n",
    "    return len(re.findall(pronoun_regex, cleaned_text, re.IGNORECASE))\n",
    "\n",
    "# Define a function to calculate the average word length\n",
    "def calculate_average_word_length(cleaned_text):\n",
    "    words = word_tokenize(cleaned_text)\n",
    "    total_characters = sum(len(word) for word in words)\n",
    "    return total_characters / len(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b187786c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31aa7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and save the derived variables for each article text\n",
    "output_data = []\n",
    "\n",
    "# Read input.xlsx to get the list of URLs and their corresponding URL_IDs\n",
    "input_file = \"Input.xlsx\"\n",
    "df_input = pd.read_excel(input_file)\n",
    "\n",
    "# Loop through each row in df_input to extract article content and save in text files\n",
    "for index, row in df_input.iterrows():\n",
    "    url_id = row[\"URL_ID\"]\n",
    "    url = row[\"URL\"]\n",
    "    article_title, article_text = extract_article_content(url)\n",
    "\n",
    "    if article_title and article_text:\n",
    "        # Save the article content in a text file\n",
    "        with open(f\"{url_id}.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(article_title + \"\\n\")\n",
    "            file.write(article_text)\n",
    "\n",
    "        # Read the content from the text file\n",
    "        with open(f\"{url_id}.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "            text = file.read()\n",
    "\n",
    "        # Clean the text using stop words\n",
    "        cleaned_text = clean_text(text, stop_words)\n",
    "\n",
    "        # Calculate the positive and negative scores\n",
    "        positive_score, negative_score = calculate_positive_negative_scores(cleaned_text, positive_words, negative_words)\n",
    "\n",
    "        # Calculate the total words after cleaning\n",
    "        total_words_after_cleaning = len(cleaned_text.split())\n",
    "\n",
    "        # Calculate the polarity score and subjectivity score\n",
    "        polarity_score, subjectivity_score = calculate_polarity_subjectivity_scores(positive_score, negative_score, total_words_after_cleaning)\n",
    "\n",
    "        # Calculate the average sentence length\n",
    "        average_sentence_length = calculate_average_sentence_length(cleaned_text)\n",
    "\n",
    "        # Calculate the percentage of complex words\n",
    "        complex_word_percentage = calculate_complex_word_percentage(cleaned_text)\n",
    "\n",
    "        # Calculate the Fog Index\n",
    "        fog_index = calculate_fog_index(average_sentence_length, complex_word_percentage)\n",
    "\n",
    "        # Calculate the average number of words per sentence\n",
    "        average_words_per_sentence = calculate_average_words_per_sentence(cleaned_text)\n",
    "\n",
    "        # Calculate the complex word count\n",
    "        complex_word_count = len([word for word in cleaned_text.split() if count_syllables(word) > 2])\n",
    "\n",
    "        # Calculate the word count\n",
    "        word_count = len(cleaned_text.split())\n",
    "\n",
    "        # Calculate the syllable count per word\n",
    "        syllable_count_per_word = sum(count_syllables(word) for word in cleaned_text.split()) / word_count\n",
    "\n",
    "        # Calculate the personal pronouns count\n",
    "        personal_pronouns_count = count_personal_pronouns(cleaned_text)\n",
    "\n",
    "        # Calculate the average word length\n",
    "        average_word_length = calculate_average_word_length(cleaned_text)\n",
    "\n",
    "        # Create a dictionary to store the derived variables for this URL\n",
    "        url_data = {\n",
    "            \"URL_ID\": url_id,\n",
    "            \"URL\": url,\n",
    "            \"Positive Score\": positive_score,\n",
    "            \"Negative Score\": negative_score,\n",
    "            \"Polarity Score\": polarity_score,\n",
    "            \"Subjectivity Score\": subjectivity_score,\n",
    "            \"Avg Sentence Length\": average_sentence_length,\n",
    "            \"Percentage of Complex Words\": complex_word_percentage,\n",
    "            \"Fog Index\": fog_index,\n",
    "            \"Avg Number of Words Per Sentence\": average_words_per_sentence,\n",
    "            \"Complex Word Count\": complex_word_count,\n",
    "            \"Word Count\": word_count,\n",
    "            \"Syllable Per Word\": syllable_count_per_word,\n",
    "            \"Personal Pronouns\": personal_pronouns_count,\n",
    "            \"Avg Word Length\": average_word_length\n",
    "        }\n",
    "        # Append the dictionary to the output_data list\n",
    "        output_data.append(url_data)\n",
    "\n",
    "# Step 5: Output Data to Excel\n",
    "\n",
    "# Create a DataFrame from the output_data list\n",
    "output_df = pd.DataFrame(output_data)\n",
    "\n",
    "# Save the DataFrame to an Excel file\n",
    "output_file = \"Output.xlsx\"\n",
    "output_df.to_excel(output_file, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3019d74c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
